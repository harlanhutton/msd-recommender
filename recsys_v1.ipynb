{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql.functions import col, explode\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '4g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '2'), ('spark.cores.max', '2'), ('spark.driver.memory','4g'),('spark.rpc.message.maxSize', '256')])\n",
    "\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "#conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "\n",
    "# Two ways you can access spark context from spark session\n",
    "sc = spark._sc\n",
    "sc2 = spark.sparkContext\n",
    "\n",
    "#spark = SparkSession.builder.config(\"spark.driver.memory\", \"16G\").getOrCreate()\n",
    "#for later --conf spark.sql.catalogImplementation=hive \n",
    "\n",
    "#sc = spark._sc\n",
    "#sc2 = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'a-10-27-16-155.dynapool.vpn.nyu.edu'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/harlanhutton/Documents/NYU%201/Second%20Sem/Big%20Data/Labs/final/final-project-recommender-systers/spark-warehouse'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.rpc.message.maxSize', '256'),\n",
       " ('spark.app.id', 'local-1620075867084'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.startTime', '1620075867018'),\n",
       " ('spark.app.name', 'Spark Updated Conf'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.cores.max', '2'),\n",
       " ('spark.driver.port', '61975'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample = spark.read.option(\"inferSchema\", True).parquet('train_sample1.parquet')\n",
    "testSample = spark.read.option(\"inferSchema\", True).parquet('test_sample1.parquet')\n",
    "trainSample.createOrReplaceTempView('trainSample')\n",
    "testSample.createOrReplaceTempView('testSample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSample = spark.read.option(\"inferSchema\", True).parquet('val_sample1.parquet')\n",
    "valSample.createOrReplaceTempView('valSample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_obj_1 = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_numer\").setHandleInvalid(\"keep\")\n",
    "indexer_model_1 = indexer_obj_1.fit(trainSample)\n",
    "indexer_df_1 = indexer_model_1.transform(trainSample)\n",
    "\n",
    "indexer_obj_2 = StringIndexer(inputCol=\"track_id\", outputCol=\"track_id_numer\").setHandleInvalid(\"keep\")\n",
    "indexer_model_2= indexer_obj_2.fit(indexer_df_1)\n",
    "indexer_df_2 = indexer_model_2.transform(indexer_df_1)\n",
    "\n",
    "train_df = indexer_df_2.drop('user_id')\n",
    "train_df= train_df.drop('track_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_1 = indexer_model_1.transform(valSample)\n",
    "val_df_2= indexer_model_2.transform(val_df_1)\n",
    "\n",
    "val_df = val_df_2.drop('user_id')\n",
    "val_df= val_df.drop('track_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_numer\", itemCol=\"track_id_numer\", ratingCol=\"count\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(train_df)\n",
    "\n",
    "#userRecs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"count\",\n",
    "#                                 predictionCol=\"prediction\")\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 song recommendations for each user\n",
    "\n",
    "### ADJUST FOR 500 SONGS \n",
    "\n",
    "# Generate top 10 user recommendations for each song\n",
    "\n",
    "#songRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS WILL BE REMOVED ####\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = train_df.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "# songs = df_dropped.select(als.getItemCol()).distinct().limit(3)\n",
    "# songSubSetRecs = model.recommendForItemSubset(songs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MIGHT NOT NEED ##\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "predictions.orderBy('prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{144345.0: [2576.0],\n",
       " 257834.0: [5251.0, 1748.0],\n",
       " 303341.0: [106998.0],\n",
       " 279263.0: [5665.0],\n",
       " 259239.0: [6919.0],\n",
       " 231190.0: [106998.0],\n",
       " 293951.0: [2662.0, 85544.0],\n",
       " 159194.0: [4265.0],\n",
       " 220558.0: [106998.0],\n",
       " 321044.0: [18028.0],\n",
       " 289159.0: [508.0],\n",
       " 120784.0: [64175.0],\n",
       " 314898.0: [21924.0, 28231.0],\n",
       " 216365.0: [23.0, 69.0, 18.0],\n",
       " 123501.0: [53918.0],\n",
       " 223776.0: [71.0],\n",
       " 147473.0: [7539.0, 25750.0],\n",
       " 75511.0: [37293.0],\n",
       " 304923.0: [22201.0],\n",
       " 261625.0: [7574.0],\n",
       " 295799.0: [40.0],\n",
       " 157069.0: [106998.0],\n",
       " 230320.0: [16226.0],\n",
       " 292319.0: [9474.0],\n",
       " 129140.0: [106998.0],\n",
       " 111524.0: [9113.0],\n",
       " 202200.0: [34735.0],\n",
       " 265663.0: [975.0],\n",
       " 290401.0: [29.0, 1.0, 9636.0],\n",
       " 103678.0: [85936.0],\n",
       " 213052.0: [12727.0],\n",
       " 112524.0: [4.0],\n",
       " 324257.0: [171.0],\n",
       " 326013.0: [14874.0],\n",
       " 309446.0: [63374.0],\n",
       " 133623.0: [1013.0],\n",
       " 270912.0: [22723.0],\n",
       " 252645.0: [9436.0],\n",
       " 76493.0: [5219.0],\n",
       " 105880.0: [7848.0],\n",
       " 305804.0: [11.0],\n",
       " 193744.0: [5300.0],\n",
       " 168872.0: [4099.0],\n",
       " 320425.0: [5439.0],\n",
       " 198823.0: [3410.0],\n",
       " 330526.0: [6544.0],\n",
       " 148406.0: [10276.0],\n",
       " 292829.0: [15983.0],\n",
       " 281776.0: [5996.0],\n",
       " 189974.0: [204.0],\n",
       " 274421.0: [9747.0, 5069.0],\n",
       " 137573.0: [2660.0],\n",
       " 199359.0: [30.0],\n",
       " 121188.0: [106998.0],\n",
       " 40677.0: [19998.0],\n",
       " 134236.0: [8223.0],\n",
       " 149838.0: [87259.0],\n",
       " 118487.0: [5172.0],\n",
       " 101001.0: [951.0],\n",
       " 267418.0: [33260.0],\n",
       " 237660.0: [139.0, 5840.0],\n",
       " 201727.0: [106998.0],\n",
       " 60031.0: [79555.0],\n",
       " 218807.0: [35499.0],\n",
       " 325362.0: [5477.0],\n",
       " 117907.0: [1237.0, 2358.0],\n",
       " 280417.0: [15382.0],\n",
       " 328537.0: [5052.0],\n",
       " 119081.0: [4599.0],\n",
       " 239287.0: [56536.0],\n",
       " 267742.0: [15081.0],\n",
       " 334889.0: [1051.0],\n",
       " 181434.0: [490.0],\n",
       " 310441.0: [30439.0],\n",
       " 126497.0: [8948.0],\n",
       " 232176.0: [435.0],\n",
       " 235323.0: [22614.0],\n",
       " 244234.0: [97545.0],\n",
       " 103476.0: [18799.0],\n",
       " 307926.0: [3192.0],\n",
       " 280329.0: [30135.0],\n",
       " 159684.0: [20968.0],\n",
       " 85555.0: [1467.0],\n",
       " 61853.0: [41724.0],\n",
       " 318792.0: [5.0],\n",
       " 242760.0: [34.0],\n",
       " 275666.0: [4109.0],\n",
       " 220628.0: [6.0],\n",
       " 216224.0: [7370.0],\n",
       " 172526.0: [4138.0],\n",
       " 192039.0: [1753.0],\n",
       " 50713.0: [19350.0],\n",
       " 260761.0: [714.0],\n",
       " 135529.0: [959.0],\n",
       " 104527.0: [2050.0],\n",
       " 180124.0: [370.0],\n",
       " 290472.0: [3645.0, 1388.0],\n",
       " 334914.0: [3555.0,\n",
       "  1374.0,\n",
       "  299.0,\n",
       "  106998.0,\n",
       "  48.0,\n",
       "  1724.0,\n",
       "  1923.0,\n",
       "  26947.0,\n",
       "  2073.0,\n",
       "  106998.0,\n",
       "  36489.0,\n",
       "  2.0,\n",
       "  4015.0,\n",
       "  28854.0,\n",
       "  162.0,\n",
       "  652.0,\n",
       "  2354.0,\n",
       "  617.0,\n",
       "  1234.0,\n",
       "  75.0,\n",
       "  187.0,\n",
       "  2125.0,\n",
       "  35709.0,\n",
       "  26500.0,\n",
       "  106998.0,\n",
       "  859.0,\n",
       "  12115.0,\n",
       "  519.0,\n",
       "  325.0,\n",
       "  73643.0,\n",
       "  40236.0,\n",
       "  4373.0,\n",
       "  106998.0,\n",
       "  19400.0,\n",
       "  4629.0,\n",
       "  13254.0,\n",
       "  12.0,\n",
       "  156.0,\n",
       "  6492.0,\n",
       "  24386.0,\n",
       "  329.0,\n",
       "  48156.0,\n",
       "  7810.0,\n",
       "  106998.0,\n",
       "  23805.0,\n",
       "  658.0,\n",
       "  32420.0,\n",
       "  28370.0,\n",
       "  392.0,\n",
       "  21923.0,\n",
       "  20835.0,\n",
       "  7803.0,\n",
       "  395.0,\n",
       "  144.0,\n",
       "  70.0,\n",
       "  106998.0,\n",
       "  4533.0,\n",
       "  130.0,\n",
       "  9165.0,\n",
       "  106998.0,\n",
       "  9098.0,\n",
       "  24616.0,\n",
       "  1510.0,\n",
       "  106998.0,\n",
       "  85459.0,\n",
       "  402.0,\n",
       "  3274.0,\n",
       "  150.0,\n",
       "  18443.0,\n",
       "  93906.0,\n",
       "  21637.0,\n",
       "  2305.0,\n",
       "  8051.0,\n",
       "  6.0,\n",
       "  833.0,\n",
       "  10207.0,\n",
       "  78.0,\n",
       "  263.0,\n",
       "  63872.0,\n",
       "  40063.0,\n",
       "  5392.0,\n",
       "  173.0,\n",
       "  24842.0,\n",
       "  635.0,\n",
       "  3630.0,\n",
       "  664.0,\n",
       "  5439.0,\n",
       "  798.0,\n",
       "  828.0,\n",
       "  68897.0,\n",
       "  3410.0,\n",
       "  14374.0,\n",
       "  28224.0,\n",
       "  163.0,\n",
       "  62717.0,\n",
       "  5275.0,\n",
       "  11380.0,\n",
       "  106998.0,\n",
       "  15594.0,\n",
       "  7563.0,\n",
       "  50682.0,\n",
       "  1064.0,\n",
       "  1943.0,\n",
       "  17006.0,\n",
       "  897.0,\n",
       "  92316.0,\n",
       "  1511.0,\n",
       "  8433.0,\n",
       "  4459.0,\n",
       "  46901.0,\n",
       "  1916.0,\n",
       "  64.0,\n",
       "  2969.0,\n",
       "  8206.0,\n",
       "  171.0,\n",
       "  6.0,\n",
       "  28.0,\n",
       "  29691.0,\n",
       "  106998.0,\n",
       "  354.0,\n",
       "  787.0,\n",
       "  21059.0,\n",
       "  106998.0,\n",
       "  1896.0,\n",
       "  106998.0,\n",
       "  1676.0,\n",
       "  106998.0,\n",
       "  76553.0,\n",
       "  106998.0,\n",
       "  6.0,\n",
       "  3125.0,\n",
       "  8410.0,\n",
       "  4469.0,\n",
       "  359.0,\n",
       "  7121.0,\n",
       "  4907.0,\n",
       "  4166.0,\n",
       "  60.0,\n",
       "  28842.0,\n",
       "  106998.0,\n",
       "  732.0,\n",
       "  18.0,\n",
       "  538.0,\n",
       "  30533.0,\n",
       "  6617.0,\n",
       "  4126.0,\n",
       "  26303.0,\n",
       "  1441.0,\n",
       "  12874.0,\n",
       "  4341.0,\n",
       "  7981.0,\n",
       "  106998.0,\n",
       "  2686.0,\n",
       "  12414.0,\n",
       "  39.0,\n",
       "  5783.0,\n",
       "  912.0,\n",
       "  104081.0,\n",
       "  13705.0,\n",
       "  79366.0,\n",
       "  782.0,\n",
       "  64.0,\n",
       "  33672.0,\n",
       "  1516.0,\n",
       "  16025.0,\n",
       "  4160.0,\n",
       "  102.0,\n",
       "  11595.0,\n",
       "  16296.0,\n",
       "  53097.0,\n",
       "  11.0,\n",
       "  49935.0,\n",
       "  510.0,\n",
       "  106998.0,\n",
       "  97101.0,\n",
       "  106998.0,\n",
       "  507.0,\n",
       "  14159.0,\n",
       "  106998.0,\n",
       "  9878.0,\n",
       "  53435.0,\n",
       "  4722.0,\n",
       "  8783.0,\n",
       "  96299.0,\n",
       "  2422.0,\n",
       "  2088.0,\n",
       "  512.0,\n",
       "  91670.0,\n",
       "  91.0,\n",
       "  561.0,\n",
       "  93145.0,\n",
       "  9711.0,\n",
       "  106998.0,\n",
       "  21596.0,\n",
       "  12743.0,\n",
       "  16792.0,\n",
       "  31589.0,\n",
       "  90.0,\n",
       "  4598.0,\n",
       "  8379.0,\n",
       "  61840.0,\n",
       "  1863.0,\n",
       "  2786.0,\n",
       "  4340.0,\n",
       "  398.0,\n",
       "  32006.0,\n",
       "  31422.0,\n",
       "  106998.0,\n",
       "  172.0,\n",
       "  37404.0,\n",
       "  4044.0,\n",
       "  80506.0,\n",
       "  3407.0,\n",
       "  31026.0,\n",
       "  4822.0,\n",
       "  7651.0,\n",
       "  106998.0,\n",
       "  22889.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  4206.0,\n",
       "  19595.0,\n",
       "  189.0,\n",
       "  97.0,\n",
       "  35816.0,\n",
       "  48926.0,\n",
       "  22712.0,\n",
       "  97299.0,\n",
       "  3917.0,\n",
       "  222.0,\n",
       "  10510.0,\n",
       "  95358.0,\n",
       "  106998.0,\n",
       "  18520.0,\n",
       "  31901.0,\n",
       "  34068.0,\n",
       "  3243.0,\n",
       "  152.0,\n",
       "  105783.0,\n",
       "  101655.0,\n",
       "  4543.0,\n",
       "  10686.0,\n",
       "  106998.0,\n",
       "  21967.0,\n",
       "  174.0,\n",
       "  184.0,\n",
       "  3412.0,\n",
       "  41583.0,\n",
       "  561.0,\n",
       "  275.0,\n",
       "  10667.0,\n",
       "  82102.0,\n",
       "  11987.0,\n",
       "  2507.0,\n",
       "  3103.0,\n",
       "  1195.0,\n",
       "  6945.0,\n",
       "  2.0,\n",
       "  6895.0,\n",
       "  7633.0,\n",
       "  86066.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  575.0,\n",
       "  154.0,\n",
       "  7020.0,\n",
       "  106998.0,\n",
       "  388.0,\n",
       "  987.0,\n",
       "  68882.0,\n",
       "  95.0,\n",
       "  96738.0,\n",
       "  3290.0,\n",
       "  32797.0,\n",
       "  2496.0,\n",
       "  106998.0,\n",
       "  10561.0,\n",
       "  389.0,\n",
       "  16881.0,\n",
       "  10682.0,\n",
       "  2422.0,\n",
       "  472.0,\n",
       "  807.0,\n",
       "  1129.0,\n",
       "  445.0,\n",
       "  331.0,\n",
       "  4261.0,\n",
       "  34072.0,\n",
       "  5303.0,\n",
       "  9051.0,\n",
       "  70.0,\n",
       "  25039.0,\n",
       "  2059.0,\n",
       "  3369.0,\n",
       "  7576.0,\n",
       "  2746.0,\n",
       "  759.0,\n",
       "  467.0,\n",
       "  5438.0,\n",
       "  4140.0,\n",
       "  14.0,\n",
       "  106998.0,\n",
       "  1.0,\n",
       "  98751.0,\n",
       "  680.0,\n",
       "  14205.0,\n",
       "  1094.0,\n",
       "  3189.0,\n",
       "  3224.0,\n",
       "  60.0,\n",
       "  2467.0,\n",
       "  1880.0,\n",
       "  46245.0,\n",
       "  79259.0,\n",
       "  40626.0,\n",
       "  397.0,\n",
       "  2714.0,\n",
       "  27965.0,\n",
       "  2976.0,\n",
       "  8500.0,\n",
       "  12909.0,\n",
       "  4656.0,\n",
       "  16718.0,\n",
       "  5630.0,\n",
       "  1777.0,\n",
       "  607.0,\n",
       "  126.0,\n",
       "  3149.0,\n",
       "  106998.0,\n",
       "  9096.0,\n",
       "  10654.0,\n",
       "  79.0,\n",
       "  13.0,\n",
       "  4559.0,\n",
       "  1308.0,\n",
       "  23.0,\n",
       "  106998.0,\n",
       "  69.0,\n",
       "  83.0,\n",
       "  57818.0,\n",
       "  106998.0,\n",
       "  2406.0,\n",
       "  1221.0,\n",
       "  778.0,\n",
       "  4773.0,\n",
       "  892.0,\n",
       "  6814.0,\n",
       "  3281.0,\n",
       "  7707.0,\n",
       "  106998.0,\n",
       "  5353.0,\n",
       "  106998.0,\n",
       "  12.0,\n",
       "  25044.0,\n",
       "  23657.0,\n",
       "  34843.0,\n",
       "  106998.0,\n",
       "  3768.0,\n",
       "  27.0,\n",
       "  30146.0,\n",
       "  971.0,\n",
       "  54965.0,\n",
       "  69735.0,\n",
       "  38349.0,\n",
       "  19.0,\n",
       "  9572.0,\n",
       "  2434.0,\n",
       "  54893.0,\n",
       "  53976.0,\n",
       "  23627.0,\n",
       "  116.0,\n",
       "  4904.0,\n",
       "  106998.0,\n",
       "  7257.0,\n",
       "  256.0,\n",
       "  10158.0,\n",
       "  2291.0,\n",
       "  5714.0,\n",
       "  10993.0,\n",
       "  31175.0,\n",
       "  23539.0,\n",
       "  28.0,\n",
       "  24540.0,\n",
       "  1233.0,\n",
       "  149.0,\n",
       "  29571.0,\n",
       "  238.0,\n",
       "  7088.0,\n",
       "  32229.0,\n",
       "  35278.0,\n",
       "  106998.0,\n",
       "  43749.0,\n",
       "  17864.0,\n",
       "  24377.0,\n",
       "  128.0,\n",
       "  40.0,\n",
       "  63775.0,\n",
       "  3586.0,\n",
       "  3742.0,\n",
       "  6916.0,\n",
       "  106998.0,\n",
       "  5073.0,\n",
       "  176.0,\n",
       "  934.0,\n",
       "  33836.0,\n",
       "  9873.0,\n",
       "  4407.0,\n",
       "  106998.0,\n",
       "  25122.0,\n",
       "  676.0,\n",
       "  16652.0,\n",
       "  5921.0,\n",
       "  2580.0,\n",
       "  8433.0,\n",
       "  22325.0,\n",
       "  89646.0,\n",
       "  20003.0,\n",
       "  86442.0,\n",
       "  21402.0,\n",
       "  106998.0,\n",
       "  217.0,\n",
       "  55.0,\n",
       "  27523.0,\n",
       "  17804.0,\n",
       "  6976.0,\n",
       "  26.0,\n",
       "  98969.0,\n",
       "  41176.0,\n",
       "  18152.0,\n",
       "  106998.0,\n",
       "  97782.0,\n",
       "  12738.0,\n",
       "  2895.0,\n",
       "  9728.0,\n",
       "  88.0,\n",
       "  223.0,\n",
       "  35.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  1900.0,\n",
       "  6485.0,\n",
       "  106998.0,\n",
       "  30502.0,\n",
       "  106998.0,\n",
       "  52.0,\n",
       "  513.0,\n",
       "  4071.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  24834.0,\n",
       "  37.0,\n",
       "  28082.0,\n",
       "  135.0,\n",
       "  35992.0,\n",
       "  1831.0,\n",
       "  11474.0,\n",
       "  5715.0,\n",
       "  8.0,\n",
       "  17685.0,\n",
       "  2189.0,\n",
       "  14514.0,\n",
       "  13079.0,\n",
       "  5051.0,\n",
       "  29191.0,\n",
       "  5517.0,\n",
       "  106998.0,\n",
       "  3844.0,\n",
       "  40441.0,\n",
       "  106998.0,\n",
       "  1786.0,\n",
       "  106998.0,\n",
       "  5032.0,\n",
       "  2778.0,\n",
       "  5730.0,\n",
       "  3592.0,\n",
       "  3726.0,\n",
       "  9338.0,\n",
       "  703.0,\n",
       "  3891.0,\n",
       "  9104.0,\n",
       "  106998.0,\n",
       "  335.0,\n",
       "  6046.0,\n",
       "  32424.0,\n",
       "  11054.0,\n",
       "  7179.0,\n",
       "  52988.0,\n",
       "  4587.0,\n",
       "  3111.0,\n",
       "  11156.0,\n",
       "  1760.0,\n",
       "  10933.0,\n",
       "  19817.0,\n",
       "  796.0,\n",
       "  7161.0,\n",
       "  106998.0,\n",
       "  536.0,\n",
       "  48793.0,\n",
       "  27489.0,\n",
       "  6830.0,\n",
       "  1316.0,\n",
       "  422.0,\n",
       "  6.0,\n",
       "  97503.0,\n",
       "  97350.0,\n",
       "  953.0,\n",
       "  53.0,\n",
       "  9485.0,\n",
       "  106998.0,\n",
       "  7769.0,\n",
       "  3176.0,\n",
       "  5000.0,\n",
       "  17823.0,\n",
       "  14738.0,\n",
       "  1339.0,\n",
       "  907.0,\n",
       "  106998.0,\n",
       "  22819.0,\n",
       "  19752.0,\n",
       "  43023.0,\n",
       "  3755.0,\n",
       "  3503.0,\n",
       "  12197.0,\n",
       "  26586.0,\n",
       "  3692.0,\n",
       "  42032.0,\n",
       "  1656.0,\n",
       "  78669.0,\n",
       "  1140.0,\n",
       "  6361.0,\n",
       "  14.0,\n",
       "  2419.0,\n",
       "  99900.0,\n",
       "  2917.0,\n",
       "  144.0,\n",
       "  12061.0,\n",
       "  2008.0,\n",
       "  2750.0,\n",
       "  27016.0,\n",
       "  52631.0,\n",
       "  2.0,\n",
       "  16.0,\n",
       "  381.0,\n",
       "  15244.0,\n",
       "  79728.0,\n",
       "  884.0,\n",
       "  32268.0,\n",
       "  106998.0,\n",
       "  3415.0,\n",
       "  1929.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  21066.0,\n",
       "  79617.0,\n",
       "  925.0,\n",
       "  14519.0,\n",
       "  8168.0,\n",
       "  2389.0,\n",
       "  106998.0,\n",
       "  3521.0,\n",
       "  18512.0,\n",
       "  106998.0,\n",
       "  39.0,\n",
       "  6396.0,\n",
       "  3058.0,\n",
       "  10141.0,\n",
       "  84.0,\n",
       "  45495.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  3343.0,\n",
       "  1419.0,\n",
       "  84.0,\n",
       "  95905.0,\n",
       "  42688.0,\n",
       "  3334.0,\n",
       "  1852.0,\n",
       "  0.0,\n",
       "  257.0,\n",
       "  7220.0,\n",
       "  25509.0,\n",
       "  16513.0,\n",
       "  5327.0,\n",
       "  2957.0,\n",
       "  1114.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  108.0,\n",
       "  106998.0,\n",
       "  107.0,\n",
       "  6872.0,\n",
       "  21290.0,\n",
       "  16334.0,\n",
       "  8500.0,\n",
       "  4628.0,\n",
       "  9060.0,\n",
       "  106998.0,\n",
       "  1314.0,\n",
       "  3289.0,\n",
       "  88677.0,\n",
       "  1448.0,\n",
       "  18899.0,\n",
       "  18047.0,\n",
       "  8643.0,\n",
       "  8.0,\n",
       "  6257.0,\n",
       "  2031.0,\n",
       "  24771.0,\n",
       "  9355.0,\n",
       "  22217.0,\n",
       "  106998.0,\n",
       "  14.0,\n",
       "  1661.0,\n",
       "  53133.0,\n",
       "  284.0,\n",
       "  51667.0,\n",
       "  1497.0,\n",
       "  17748.0,\n",
       "  1079.0,\n",
       "  13738.0,\n",
       "  967.0,\n",
       "  855.0,\n",
       "  106998.0,\n",
       "  56.0,\n",
       "  78.0,\n",
       "  77746.0,\n",
       "  17392.0,\n",
       "  8291.0,\n",
       "  8089.0,\n",
       "  9373.0,\n",
       "  36735.0,\n",
       "  5541.0,\n",
       "  13075.0,\n",
       "  45337.0,\n",
       "  16.0,\n",
       "  39988.0,\n",
       "  21662.0,\n",
       "  8141.0,\n",
       "  4228.0,\n",
       "  41455.0,\n",
       "  308.0,\n",
       "  15419.0,\n",
       "  1377.0,\n",
       "  2777.0,\n",
       "  12217.0,\n",
       "  1002.0,\n",
       "  8842.0,\n",
       "  22.0,\n",
       "  94164.0,\n",
       "  4084.0,\n",
       "  10.0,\n",
       "  21241.0,\n",
       "  4403.0,\n",
       "  60433.0,\n",
       "  1315.0,\n",
       "  277.0,\n",
       "  106998.0,\n",
       "  31888.0,\n",
       "  13284.0,\n",
       "  6829.0,\n",
       "  21958.0,\n",
       "  78436.0,\n",
       "  21631.0,\n",
       "  4528.0,\n",
       "  4475.0,\n",
       "  5699.0,\n",
       "  12566.0,\n",
       "  90938.0,\n",
       "  3908.0,\n",
       "  47809.0,\n",
       "  15934.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  267.0,\n",
       "  3055.0,\n",
       "  27.0,\n",
       "  10435.0,\n",
       "  106998.0,\n",
       "  10160.0,\n",
       "  13573.0,\n",
       "  20840.0,\n",
       "  18426.0,\n",
       "  356.0,\n",
       "  55.0,\n",
       "  106998.0,\n",
       "  96.0,\n",
       "  6720.0,\n",
       "  21203.0,\n",
       "  252.0,\n",
       "  1113.0,\n",
       "  39029.0,\n",
       "  13598.0,\n",
       "  1490.0,\n",
       "  90.0,\n",
       "  150.0,\n",
       "  23.0,\n",
       "  3047.0,\n",
       "  21255.0,\n",
       "  1272.0,\n",
       "  2371.0,\n",
       "  2.0,\n",
       "  37669.0,\n",
       "  19811.0,\n",
       "  106998.0,\n",
       "  7346.0,\n",
       "  35441.0,\n",
       "  549.0,\n",
       "  7186.0,\n",
       "  1618.0,\n",
       "  1399.0,\n",
       "  16044.0,\n",
       "  1580.0,\n",
       "  5193.0,\n",
       "  696.0,\n",
       "  2720.0,\n",
       "  3554.0,\n",
       "  47449.0,\n",
       "  4133.0,\n",
       "  106998.0,\n",
       "  1117.0,\n",
       "  1398.0,\n",
       "  40337.0,\n",
       "  1216.0,\n",
       "  37233.0,\n",
       "  2649.0,\n",
       "  771.0,\n",
       "  7011.0,\n",
       "  9335.0,\n",
       "  18935.0,\n",
       "  106998.0,\n",
       "  5608.0,\n",
       "  391.0,\n",
       "  6506.0,\n",
       "  5937.0,\n",
       "  104190.0,\n",
       "  2872.0,\n",
       "  2791.0,\n",
       "  57660.0,\n",
       "  68855.0,\n",
       "  104981.0,\n",
       "  106998.0,\n",
       "  3467.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  32887.0,\n",
       "  1323.0,\n",
       "  1584.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  18253.0,\n",
       "  35768.0,\n",
       "  34997.0,\n",
       "  11048.0,\n",
       "  812.0,\n",
       "  6966.0,\n",
       "  481.0,\n",
       "  40781.0,\n",
       "  2204.0,\n",
       "  1464.0,\n",
       "  22479.0,\n",
       "  3054.0,\n",
       "  23960.0,\n",
       "  7025.0,\n",
       "  22276.0,\n",
       "  5917.0,\n",
       "  14811.0,\n",
       "  99918.0,\n",
       "  4052.0,\n",
       "  2068.0,\n",
       "  277.0,\n",
       "  702.0,\n",
       "  35255.0,\n",
       "  29.0,\n",
       "  18832.0,\n",
       "  28839.0,\n",
       "  7091.0,\n",
       "  1341.0,\n",
       "  1.0,\n",
       "  139.0,\n",
       "  17965.0,\n",
       "  24812.0,\n",
       "  72660.0,\n",
       "  18.0,\n",
       "  106998.0,\n",
       "  83.0,\n",
       "  106998.0,\n",
       "  15841.0,\n",
       "  4209.0,\n",
       "  3099.0,\n",
       "  30.0,\n",
       "  99498.0,\n",
       "  24021.0,\n",
       "  61.0,\n",
       "  106998.0,\n",
       "  51126.0,\n",
       "  2688.0,\n",
       "  233.0,\n",
       "  375.0,\n",
       "  970.0,\n",
       "  238.0,\n",
       "  83862.0,\n",
       "  83395.0,\n",
       "  60520.0,\n",
       "  2531.0,\n",
       "  2402.0,\n",
       "  609.0,\n",
       "  106998.0,\n",
       "  4609.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  54939.0,\n",
       "  0.0,\n",
       "  4162.0,\n",
       "  2231.0,\n",
       "  8062.0,\n",
       "  1035.0,\n",
       "  27644.0,\n",
       "  106998.0,\n",
       "  5037.0,\n",
       "  1603.0,\n",
       "  12026.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  14.0,\n",
       "  2787.0,\n",
       "  56.0,\n",
       "  106998.0,\n",
       "  910.0,\n",
       "  850.0,\n",
       "  30.0,\n",
       "  6170.0,\n",
       "  100262.0,\n",
       "  22182.0,\n",
       "  6923.0,\n",
       "  46088.0,\n",
       "  1964.0,\n",
       "  33682.0,\n",
       "  792.0,\n",
       "  22554.0,\n",
       "  47921.0,\n",
       "  72217.0,\n",
       "  856.0,\n",
       "  830.0,\n",
       "  27894.0,\n",
       "  574.0,\n",
       "  117.0,\n",
       "  118.0,\n",
       "  43480.0,\n",
       "  1042.0,\n",
       "  3863.0,\n",
       "  88.0,\n",
       "  7088.0,\n",
       "  9684.0,\n",
       "  106998.0,\n",
       "  34029.0,\n",
       "  3467.0,\n",
       "  17275.0,\n",
       "  23429.0,\n",
       "  7378.0,\n",
       "  73.0,\n",
       "  84215.0,\n",
       "  109.0,\n",
       "  103352.0,\n",
       "  20361.0,\n",
       "  2153.0,\n",
       "  9046.0,\n",
       "  2031.0,\n",
       "  9057.0,\n",
       "  38368.0,\n",
       "  48.0,\n",
       "  102237.0,\n",
       "  106998.0,\n",
       "  92362.0,\n",
       "  3734.0,\n",
       "  114.0,\n",
       "  26818.0,\n",
       "  401.0,\n",
       "  15274.0,\n",
       "  68326.0,\n",
       "  13100.0,\n",
       "  8331.0,\n",
       "  25892.0,\n",
       "  2519.0,\n",
       "  133.0,\n",
       "  8828.0,\n",
       "  2891.0,\n",
       "  106998.0,\n",
       "  14.0,\n",
       "  6956.0,\n",
       "  334.0,\n",
       "  10302.0,\n",
       "  10957.0,\n",
       "  13873.0,\n",
       "  35506.0,\n",
       "  19075.0,\n",
       "  3855.0,\n",
       "  106998.0,\n",
       "  365.0,\n",
       "  1166.0,\n",
       "  13959.0,\n",
       "  0.0,\n",
       "  850.0,\n",
       "  7071.0,\n",
       "  135.0,\n",
       "  25145.0,\n",
       "  32824.0,\n",
       "  192.0,\n",
       "  407.0,\n",
       "  2203.0,\n",
       "  3632.0,\n",
       "  1198.0,\n",
       "  2586.0,\n",
       "  11697.0,\n",
       "  286.0,\n",
       "  68.0,\n",
       "  119.0,\n",
       "  106998.0,\n",
       "  5167.0,\n",
       "  955.0,\n",
       "  582.0,\n",
       "  7441.0,\n",
       "  106998.0,\n",
       "  440.0,\n",
       "  106998.0,\n",
       "  80080.0,\n",
       "  42.0,\n",
       "  4268.0,\n",
       "  22401.0,\n",
       "  15365.0,\n",
       "  16423.0,\n",
       "  68741.0,\n",
       "  150.0,\n",
       "  23498.0,\n",
       "  6896.0,\n",
       "  2501.0,\n",
       "  5390.0,\n",
       "  59441.0,\n",
       "  18333.0,\n",
       "  14452.0,\n",
       "  27127.0,\n",
       "  11573.0,\n",
       "  20109.0,\n",
       "  55569.0,\n",
       "  6590.0,\n",
       "  501.0,\n",
       "  12107.0,\n",
       "  106998.0,\n",
       "  325.0,\n",
       "  9585.0,\n",
       "  164.0,\n",
       "  12732.0,\n",
       "  5288.0,\n",
       "  14642.0,\n",
       "  27791.0,\n",
       "  65.0,\n",
       "  10348.0,\n",
       "  354.0,\n",
       "  415.0,\n",
       "  269.0,\n",
       "  6945.0,\n",
       "  412.0,\n",
       "  179.0,\n",
       "  19911.0,\n",
       "  1034.0,\n",
       "  11120.0,\n",
       "  995.0,\n",
       "  1224.0,\n",
       "  13927.0,\n",
       "  8122.0,\n",
       "  106998.0,\n",
       "  304.0,\n",
       "  106998.0,\n",
       "  1946.0,\n",
       "  106998.0,\n",
       "  5775.0,\n",
       "  106998.0,\n",
       "  25907.0,\n",
       "  106998.0,\n",
       "  25185.0,\n",
       "  106998.0,\n",
       "  106998.0,\n",
       "  12551.0,\n",
       "  5638.0,\n",
       "  18414.0,\n",
       "  77666.0,\n",
       "  450.0,\n",
       "  8104.0,\n",
       "  22789.0,\n",
       "  1654.0,\n",
       "  17294.0,\n",
       "  31191.0,\n",
       "  40809.0,\n",
       "  1571.0,\n",
       "  2883.0,\n",
       "  19126.0,\n",
       "  1228.0,\n",
       "  977.0,\n",
       "  524.0,\n",
       "  ...],\n",
       " 158350.0: [3542.0],\n",
       " 217733.0: [412.0],\n",
       " 297135.0: [106998.0],\n",
       " 87678.0: [20240.0, 106998.0],\n",
       " 171418.0: [30998.0],\n",
       " 218761.0: [24816.0],\n",
       " 259837.0: [17078.0],\n",
       " 155416.0: [16815.0],\n",
       " 89150.0: [6270.0, 55827.0],\n",
       " 291520.0: [46568.0],\n",
       " 63452.0: [106998.0],\n",
       " 104610.0: [56086.0],\n",
       " 317063.0: [3996.0],\n",
       " 202168.0: [340.0],\n",
       " 323823.0: [75104.0],\n",
       " 326772.0: [18967.0],\n",
       " 231535.0: [22103.0, 7705.0],\n",
       " 171332.0: [395.0],\n",
       " 235684.0: [106998.0],\n",
       " 80027.0: [9194.0],\n",
       " 109509.0: [106998.0],\n",
       " 231533.0: [106998.0, 9207.0],\n",
       " 146868.0: [422.0],\n",
       " 208594.0: [558.0],\n",
       " 84792.0: [10409.0],\n",
       " 109977.0: [2995.0],\n",
       " 329174.0: [106998.0, 106998.0],\n",
       " 253436.0: [6275.0],\n",
       " 264172.0: [14729.0],\n",
       " 294737.0: [16600.0],\n",
       " 263791.0: [26258.0],\n",
       " 273951.0: [74.0, 6.0],\n",
       " 308309.0: [12526.0],\n",
       " 33592.0: [1269.0],\n",
       " 39790.0: [43290.0],\n",
       " 154249.0: [68975.0],\n",
       " 239392.0: [1947.0],\n",
       " 266502.0: [63.0],\n",
       " 267537.0: [81571.0],\n",
       " 18182.0: [1292.0],\n",
       " 257274.0: [106998.0],\n",
       " 261238.0: [10712.0],\n",
       " 234367.0: [0.0],\n",
       " 108267.0: [16976.0, 5567.0],\n",
       " 218911.0: [1760.0],\n",
       " 291932.0: [69634.0],\n",
       " 201552.0: [4881.0],\n",
       " 104579.0: [202.0],\n",
       " 132144.0: [12001.0],\n",
       " 162970.0: [11135.0],\n",
       " 188171.0: [102.0],\n",
       " 300367.0: [6452.0],\n",
       " 154731.0: [2454.0],\n",
       " 79150.0: [1432.0],\n",
       " 234520.0: [2962.0],\n",
       " 78014.0: [106998.0],\n",
       " 302361.0: [106998.0],\n",
       " 197458.0: [2938.0],\n",
       " 268121.0: [5253.0],\n",
       " 265115.0: [106998.0],\n",
       " 157669.0: [197.0],\n",
       " 246976.0: [55277.0],\n",
       " 70922.0: [7856.0, 10253.0],\n",
       " 332747.0: [67999.0],\n",
       " 212342.0: [124.0],\n",
       " 159657.0: [13309.0],\n",
       " 171297.0: [37768.0],\n",
       " 108774.0: [26177.0],\n",
       " 87357.0: [22332.0],\n",
       " 183072.0: [13655.0],\n",
       " 200712.0: [2407.0],\n",
       " 221428.0: [106998.0],\n",
       " 185699.0: [106998.0],\n",
       " 159151.0: [106998.0],\n",
       " 273807.0: [4372.0],\n",
       " 317769.0: [27.0],\n",
       " 188258.0: [894.0],\n",
       " 302543.0: [106998.0],\n",
       " 269225.0: [71888.0],\n",
       " 133760.0: [10185.0],\n",
       " 292472.0: [581.0],\n",
       " 290948.0: [0.0],\n",
       " 232158.0: [9220.0],\n",
       " 279894.0: [10712.0],\n",
       " 283141.0: [28130.0],\n",
       " 96197.0: [51098.0],\n",
       " 264326.0: [17992.0],\n",
       " 100848.0: [31826.0],\n",
       " 199319.0: [7797.0],\n",
       " 125274.0: [121.0],\n",
       " 154833.0: [106998.0],\n",
       " 260240.0: [106998.0],\n",
       " 324522.0: [1869.0],\n",
       " 45447.0: [112.0],\n",
       " 287868.0: [506.0]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each user, sort track ids by count\n",
    "val_true = val_df.orderBy('count')\n",
    "\n",
    "# flatten to group by user id and get list of true track ids\n",
    "val_true_flatten = val_true.groupby('user_id_numer').agg(func.collect_list('track_id_numer').alias(\"track_id_numer\"))\n",
    "\n",
    "# add to dictionary\n",
    "val_true_dict = val_true_flatten.collect()\n",
    "val_true_dict = [{r['user_id_numer']: r['track_id_numer']} for r in val_true_dict]\n",
    "val_true_dict = dict((key,d[key]) for d in val_true_dict for key in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59390481/how-to-implement-ranking-metrics-of-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.recommendForUserSubset(val_df,1)\n",
    "val_preds_explode = val_preds.select(val_preds.user_id_numer,explode(val_preds.recommendations.track_id_numer))\n",
    "\n",
    "val_preds_flatten = val_preds_explode.groupby('user_id_numer').agg(func.collect_list('col').alias(\"col\"))\n",
    "\n",
    "val_preds_dict = val_preds_flatten.collect()\n",
    "val_preds_dict = [{r['user_id_numer']: r['col']} for r in val_preds_dict]\n",
    "val_preds_dict = dict((key,d[key]) for d in val_preds_dict for key in d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (a-10-27-16-155.dynapool.vpn.nyu.edu executor driver): org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor183.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-48cd37e4a41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRankingMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(metrics.meanAveragePrecision)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/mllib/evaluation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, predictionAndLabels)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0msql_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         df = sql_ctx.createDataFrame(predictionAndLabels,\n\u001b[0;32m--> 436\u001b[0;31m                                      schema=sql_ctx._inferSchema(predictionAndLabels))\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"newRankingMetrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRankingMetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (a-10-27-16-155.dynapool.vpn.nyu.edu executor driver): org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor183.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "\n",
    "for user in val_preds_dict.keys():\n",
    "    labels_list.append((val_preds_dict[user], [int(i) for i in val_true_dict[user]]))\n",
    "\n",
    "labels = sc.parallelize(labels_list)\n",
    "metrics = RankingMetrics(labels)\n",
    "#print(metrics.meanAveragePrecision)\n",
    "    \n",
    "# add predicted items and true items as tuples to list\n",
    "\n",
    "# parallelize list\n",
    "# call ranking metrics on list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([28588], [5219]),\n",
       " ([31777], [63]),\n",
       " ([12756], [18967]),\n",
       " ([54627], [56086]),\n",
       " ([11827], [9113]),\n",
       " ([11827], [19998]),\n",
       " ([48884], [370]),\n",
       " ([28588], [7370]),\n",
       " ([28588], [9436]),\n",
       " ([28588], [18799]),\n",
       " ([31449], [412]),\n",
       " ([26092], [75104]),\n",
       " ([28588], [26177]),\n",
       " ([12756], [435]),\n",
       " ([11827], [395]),\n",
       " ([25176], [43290]),\n",
       " ([48884], [22201]),\n",
       " ([11827], [508]),\n",
       " ([11827], [10185]),\n",
       " ([48884], [5665]),\n",
       " ([54627], [14729]),\n",
       " ([48884], [106998]),\n",
       " ([12756], [29, 1, 9636]),\n",
       " ([54627], [5251, 1748]),\n",
       " ([11827], [4138]),\n",
       " ([28588], [6]),\n",
       " ([42403], [1869]),\n",
       " ([10472], [22614]),\n",
       " ([50608], [15983]),\n",
       " ([50608], [5477]),\n",
       " ([37291], [7539, 25750]),\n",
       " ([28588], [85936]),\n",
       " ([11827], [106998]),\n",
       " ([10896], [63374]),\n",
       " ([48884], [6544]),\n",
       " ([11827], [202]),\n",
       " ([1757], [7574]),\n",
       " ([36349], [19350]),\n",
       " ([31449], [4109]),\n",
       " ([50608], [10276]),\n",
       " ([9831], [4099]),\n",
       " ([31449], [81571]),\n",
       " ([49471], [2962]),\n",
       " ([9831], [3410]),\n",
       " ([31777], [490]),\n",
       " ([31340], [6275]),\n",
       " ([26092], [581]),\n",
       " ([12756], [139, 5840]),\n",
       " ([1757], [16226]),\n",
       " ([54627], [79555]),\n",
       " ([12756], [15382]),\n",
       " ([16901], [20968]),\n",
       " ([11827], [951]),\n",
       " ([50608], [2050]),\n",
       " ([26092], [4372]),\n",
       " ([11827], [68975]),\n",
       " ([12756], [197]),\n",
       " ([28588], [106998]),\n",
       " ([1757], [5996]),\n",
       " ([1757], [37768]),\n",
       " ([25176], [9474]),\n",
       " ([1757], [340]),\n",
       " ([11827], [2995]),\n",
       " ([1757], [16815]),\n",
       " ([11827], [28130]),\n",
       " ([28588], [6270, 55827]),\n",
       " ([28588], [714]),\n",
       " ([11827], [97545]),\n",
       " ([28588], [106998]),\n",
       " ([1757], [106998]),\n",
       " ([48884], [506]),\n",
       " ([40636], [204]),\n",
       " ([62081], [18028]),\n",
       " ([31449], [56536]),\n",
       " ([1757], [22332]),\n",
       " ([11827], [15081]),\n",
       " ([12756], [1467]),\n",
       " ([28588], [5439]),\n",
       " ([15646], [106998]),\n",
       " ([34774], [124]),\n",
       " ([48884], [894]),\n",
       " ([48884], [10712]),\n",
       " ([1757], [13655]),\n",
       " ([48884], [3996]),\n",
       " ([48884], [20240, 106998]),\n",
       " ([10896], [106998]),\n",
       " ([16999], [51098]),\n",
       " ([31449], [7848]),\n",
       " ([9831], [22723]),\n",
       " ([11827], [4]),\n",
       " ([28588], [106998]),\n",
       " ([15646], [41724]),\n",
       " ([11827], [3192]),\n",
       " ([45078], [2454]),\n",
       " ([35076], [0]),\n",
       " ([12756], [69634]),\n",
       " ([50608], [23, 69, 18]),\n",
       " ([10472], [21924, 28231]),\n",
       " ([11827], [7856, 10253]),\n",
       " ([48884], [106998]),\n",
       " ([11827], [40]),\n",
       " ([48884], [74, 6]),\n",
       " ([50608], [13309]),\n",
       " ([48884], [3542]),\n",
       " ([1757], [106998]),\n",
       " ([54627], [30135]),\n",
       " ([28588], [11]),\n",
       " ([54627], [34]),\n",
       " ([48884], [71888]),\n",
       " ([10472], [106998]),\n",
       " ([11827], [53918]),\n",
       " ([1757], [17992]),\n",
       " ([12756], [2662, 85544]),\n",
       " ([9831], [9194]),\n",
       " ([31777], [11135]),\n",
       " ([11827], [71]),\n",
       " ([54627], [975]),\n",
       " ([1757], [2407]),\n",
       " ([1757], [106998]),\n",
       " ([11827], [2576]),\n",
       " ([54627], [55277]),\n",
       " ([54627], [106998]),\n",
       " ([15646], [6919]),\n",
       " ([11827], [24816]),\n",
       " ([1757], [1753]),\n",
       " ([11827], [106998]),\n",
       " ([11827], [33260]),\n",
       " ([3595], [102]),\n",
       " ([37291], [6452]),\n",
       " ([62081], [8948]),\n",
       " ([54627], [12001]),\n",
       " ([4576], [5300]),\n",
       " ([48884], [46568]),\n",
       " ([54627], [2938]),\n",
       " ([54627], [10712]),\n",
       " ([11827], [106998]),\n",
       " ([1757], [87259]),\n",
       " ([48884], [26258]),\n",
       " ([4576], [5172]),\n",
       " ([50608], [34735]),\n",
       " ([15646], [7797]),\n",
       " ([12756], [64175]),\n",
       " ([48884], [106998, 9207]),\n",
       " ([54627], [30998]),\n",
       " ([52190], [8223]),\n",
       " ([16999], [0]),\n",
       " ([31449], [106998]),\n",
       " ([1757], [4881]),\n",
       " ([28588], [1947]),\n",
       " ([54627], [12727]),\n",
       " ([28588], [14874]),\n",
       " ([48884], [5]),\n",
       " ([11827], [171]),\n",
       " ([11827], [106998]),\n",
       " ([1757], [5253]),\n",
       " ([11827], [106998]),\n",
       " ([54627], [106998]),\n",
       " ([9831], [112]),\n",
       " ([54627], [12526]),\n",
       " ([42403], [2660]),\n",
       " ([28588], [22103, 7705]),\n",
       " ([46195], [959]),\n",
       " ([54627], [37293]),\n",
       " ([12756], [4599]),\n",
       " ([48884], [10409]),\n",
       " ([50608], [16976, 5567]),\n",
       " ([11827], [27]),\n",
       " ([50608], [1432]),\n",
       " ([31340], [35499]),\n",
       " ([54627], [422]),\n",
       " ([11827], [9220]),\n",
       " ([10472], [1292]),\n",
       " ([14473], [5052]),\n",
       " ([54627], [31826]),\n",
       " ([54627], [16600]),\n",
       " ([48884], [30439]),\n",
       " ([26092], [1237, 2358]),\n",
       " ([71366], [9747, 5069]),\n",
       " ([12756], [106998]),\n",
       " ([11827], [4265]),\n",
       " ([28588], [1269]),\n",
       " ([11827], [3645, 1388]),\n",
       " ([11827], [1760]),\n",
       " ([54627], [30]),\n",
       " ([54627], [1051]),\n",
       " ([35076], [121]),\n",
       " ([54627], [558]),\n",
       " ([28588], [1013]),\n",
       " ([11827], [106998, 106998]),\n",
       " ([54627], [17078]),\n",
       " ([10472], [106998]),\n",
       " ([11827], [67999])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaModelWrapper.__del__ at 0x7fe3e01be160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/mllib/common.py\", line 137, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "AttributeError: 'RankingMetrics' object has no attribute '_sc'\n",
      "Exception ignored in: <function JavaModelWrapper.__del__ at 0x7fe3e01be160>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/mllib/common.py\", line 137, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "AttributeError: 'RankingMetrics' object has no attribute '_sc'\n"
     ]
    }
   ],
   "source": [
    "userRecommended = model.recommendForUserSubset(val_df, 10).collect()\n",
    "userRecommended_RDD = sc.parallelize(userRecommended).map(lambda k,v: (k, [x[1] for x in v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 162.0 failed 1 times, most recent failure: Lost task 7.0 in stage 162.0 (TID 1837) (a-10-27-16-155.dynapool.vpn.nyu.edu executor driver): org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-fff4533fdf9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muserRecommended_RDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 162.0 failed 1 times, most recent failure: Lost task 7.0 in stage 162.0 (TID 1837) (a-10-27-16-155.dynapool.vpn.nyu.edu executor driver): org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  dyld: Library not loaded: /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation\n    Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python\n    Reason: image not found\nPYTHONPATH was:\n  /Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9-src.zip:/Users/harlanhutton/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.1.1.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 134.\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:221)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:132)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:105)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "userRecommended_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecommended = model.recommendProductsForUsers(10).collect()\n",
    "userRecommended_RDD = sc.parallelize(userRecommended)\\\n",
    "                         .map(lambda k,v: (k, [x[1] for x in v]))\n",
    "\n",
    "# Get the actual usage\n",
    "userMovies = validation_RDD.groupBy(\"new_uid\")\\\n",
    "    .agg(F.collect_set(\"new_pgm\")\\\n",
    "    .alias(\"actualVideos\")).rdd\n",
    "\n",
    "# join into a prediction vs actual RDD\n",
    "predictionsAndLabels = userRecommended_RDD.join(userMovies).cache()\n",
    "\n",
    "# Get the metrics\n",
    "metricsRank = RankingMetrics(predictionsAndLabels.map(lambda r: r[1]))\n",
    "tt_adv = time() - t0_adv\n",
    "\n",
    "print \"Time required : %.0f\" % tt_adv\n",
    "print \"p5   %.8f\" % metricsRank.precisionAt(5)\n",
    "print \"MAP  %.8f\" % metricsRank.meanAveragePrecision\n",
    "print \"nDCG %.8f\" % metricsRank.ndcgAt(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-2-ae4c2209be47>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-b4814332865b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"count app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m words = sc.parallelize (\n\u001b[1;32m      4\u001b[0m    [\"scala\", \n\u001b[1;32m      5\u001b[0m    \u001b[0;34m\"java\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by getOrCreate at <ipython-input-2-ae4c2209be47>:3 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"count app\")\n",
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
