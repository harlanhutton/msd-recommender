{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql.functions import col, explode\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Import the requisite items\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_201\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_201-b09)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "! java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16G\").getOrCreate()\n",
    "\n",
    "sc = spark._sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSample = spark.read.option(\"inferSchema\", True).parquet('train_sample1.parquet')\n",
    "testSample = spark.read.option(\"inferSchema\", True).parquet('test_sample1.parquet')\n",
    "trainSample.createOrReplaceTempView('trainSample')\n",
    "testSample.createOrReplaceTempView('testSample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSample = spark.read.option(\"inferSchema\", True).parquet('val_sample1.parquet')\n",
    "valSample.createOrReplaceTempView('valSample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_obj_1 = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_numer\").setHandleInvalid(\"keep\")\n",
    "indexer_model_1 = indexer_obj_1.fit(trainSample)\n",
    "indexer_df_1 = indexer_model_1.transform(trainSample)\n",
    "\n",
    "indexer_obj_2 = StringIndexer(inputCol=\"track_id\", outputCol=\"track_id_numer\").setHandleInvalid(\"keep\")\n",
    "indexer_model_2 = indexer_obj_2.fit(indexer_df_1)\n",
    "indexer_df_2 = indexer_model_2.transform(indexer_df_1)\n",
    "\n",
    "\n",
    "train_df = indexer_df_2.drop('user_id')\n",
    "train_df = train_df.drop('track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_1 = indexer_model_1.transform(valSample)\n",
    "val_df_2 = indexer_model_2.transform(val_df_1)\n",
    "\n",
    "val_df = val_df_2.drop('user_id')\n",
    "val_df = val_df.drop('track_id')\n",
    "\n",
    "test_df_1 = indexer_model_1.transform(testSample)\n",
    "test_df_2 = indexer_model_2.transform(test_df_1)\n",
    "\n",
    "test_df = test_df_2.drop('user_id')\n",
    "test_df = test_df.drop('track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "# als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_numer\", itemCol=\"track_id_numer\", ratingCol= \"count\",\n",
    "#           coldStartStrategy=\"drop\", implicitPrefs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparam Tuning\n",
    "from bayes_opt import BayesianOptimization\n",
    "tuning_params = dict()\n",
    "tuning_params = {\"rank\":(30,70),\"maxIter\":(8,16),\"regParam\":(.01,1),\"alpha\":(0.0,3.0)}\n",
    "def BO_func(rank,maxIter,regParam,alpha):\n",
    "    recommender = ALS(userCol=\"user_id_numer\",itemCol=\"track_id_numer\",ratingCol=\"count\",\n",
    "                     coldStartStrategy=\"drop\",implicitPrefs=True,rank=int(rank),\n",
    "                     maxIter=int(maxIter),regParam=int(regParam),alpha=int(alpha))\n",
    "    model = recommender.fit(train_df)\n",
    "    preds = model.transform(val_df)\n",
    "    res_valid = RegressionEvaluator(metricName=\"rmse\",labelCol=\"count\",\n",
    "                                   predictionCol=\"prediction\")\n",
    "    rmse=res_valid.evaluate(preds)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |  maxIter  |   rank    | regParam  |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 4.662   \u001b[0m | \u001b[0m 2.222   \u001b[0m | \u001b[0m 11.48   \u001b[0m | \u001b[0m 42.07   \u001b[0m | \u001b[0m 0.9194  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 4.666   \u001b[0m | \u001b[95m 2.488   \u001b[0m | \u001b[95m 10.45   \u001b[0m | \u001b[95m 47.66   \u001b[0m | \u001b[95m 0.5232  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 4.662   \u001b[0m | \u001b[0m 2.755   \u001b[0m | \u001b[0m 11.38   \u001b[0m | \u001b[0m 42.2    \u001b[0m | \u001b[0m 0.1176  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 4.663   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target': 4.665940743841398,\n",
       " 'params': {'alpha': 2.4884111887948293,\n",
       "  'maxIter': 10.446975451610584,\n",
       "  'rank': 47.65907856480315,\n",
       "  'regParam': 0.5232338079942138}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer  = BayesianOptimization(\n",
    "f=BO_func,\n",
    "pbounds=tuning_params,\n",
    "verbose=5,\n",
    "random_state=5,\n",
    ")\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "n_iter=5,\n",
    ")\n",
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8119e043d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#type(optimizer.max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmaxIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maxIter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "#type(optimizer.max)\n",
    "params = optimizer.max.get('params')\n",
    "alpha = params.get(\"alpha\")\n",
    "rank = params.get(\"rank\")\n",
    "maxIter = params.get(\"maxIter\")\n",
    "regParam= params.get(\"regParam\")\n",
    "#regParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement with optimal hyperparameters\n",
    "recommender = ALS(userCol=\"user_id_numer\",itemCol=\"track_id_numer\",ratingCol=\"count\",\n",
    "                     coldStartStrategy=\"drop\",implicitPrefs=True,rank=int(rank),\n",
    "                     maxIter=float(maxIter),regParam=float(regParam),alpha=float(alpha))\n",
    "model = recommender.fit(train_df)\n",
    "#change the val_df to test\n",
    "val_transformed = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_transformed = model.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best_model\n",
    "#print(type(best_model))\n",
    "\n",
    "# Complete the code below to extract the ALS model parameters\n",
    "#print(\"**Best Model**\")\n",
    "\n",
    "# # Print \"Rank\"\n",
    "#print(\"  Rank:\", best_model._java_obj.parent().getRank())\n",
    "\n",
    "# Print \"MaxIter\"\n",
    "#print(\"  MaxIter:\", best_model._java_obj.parent().getMaxIter())\n",
    "\n",
    "# Print \"RegParam\"\n",
    "#print(\"  RegParam:\", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user, sort track ids by count\n",
    "val_true = val_df.orderBy('count')\n",
    "\n",
    "# flatten to group by user id and get list of true track ids\n",
    "val_true_flatten = val_true.groupby('user_id_numer').agg(func.collect_list('track_id_numer').alias(\"track_id_numer\"))\n",
    "\n",
    "# add to dictionary\n",
    "val_true_dict = val_true_flatten.collect()\n",
    "val_true_dict = [{r['user_id_numer']: r['track_id_numer']} for r in val_true_dict]\n",
    "val_true_dict = dict((key,d[key]) for d in val_true_dict for key in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_true_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59390481/how-to-implement-ranking-metrics-of-pyspark\n",
    "#https://stackoverflow.com/questions/67345691/apply-stringindexer-to-several-columns-in-multiple-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model transform before recommend for UserSubset\n",
    "### recommend for distinct users in validation\n",
    "### implicit prefs = true ???\n",
    "users = val_transformed.select(als.getUserCol()).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.recommendForUserSubset(users, 10)\n",
    "val_preds_explode = val_preds.select(val_preds.user_id_numer,explode(val_preds.recommendations.track_id_numer))\n",
    "\n",
    "val_preds_flatten = val_preds_explode.groupby('user_id_numer').agg(func.collect_list('col').alias(\"col\"))\n",
    "\n",
    "val_preds_dict = val_preds_flatten.collect()\n",
    "val_preds_dict = [{r['user_id_numer']: r['col']} for r in val_preds_dict]\n",
    "val_preds_dict = dict((key,d[key]) for d in val_preds_dict for key in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--spark.yarn.submit.file.replication=1  --> replication factor\n",
    "\n",
    "dictcon= list(map(list, val_preds_dict.items()))\n",
    "dfpreds = spark.createDataFrame(dictcon, [\"user_id_numer\", \"tracks\"])\n",
    "\n",
    "dictcon2= list(map(list, val_true_dict.items()))\n",
    "dftrue = spark.createDataFrame(dictcon2, [\"user_id_numer\", \"tracks\"])\n",
    "\n",
    "rankingsRDD = (dfpreds.join(dftrue, 'user_id_numer')\n",
    "               .rdd\n",
    "               .map(lambda row: (row[1], row[2])))\n",
    "rankingsRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = RankingMetrics(rankingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD ### \n",
    "\n",
    "# labels_list = []\n",
    "\n",
    "# for user in val_preds_dict.keys():\n",
    "#     labels_list.append((val_preds_dict[user], [int(i) for i in val_true_dict[user]]))\n",
    "\n",
    "# labels = sc.parallelize(labels_list)\n",
    "metrics = RankingMetrics(rankingsRDD)\n",
    "#print(metrics.meanAveragePrecision)# metrics = RankingMetrics(labels)\n",
    "# #print(metrics.meanAveragePrecision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
